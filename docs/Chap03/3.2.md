## 3.2-1

> Seja $f(n) + g(n)$ funções assintoticamente não negativas. Usando a definição básica da notação $\Theta$, prove que $\max(f(n), g(n)) = \Theta(f(n) + g(n))$.

Para provar que $\max(f(n), g(n)) = \Theta(f(n) + g(n))$, precisamos mostrar que $\max(f(n), g(n))$ é limitado superiormente e inferiormente por $f(n) + g(n)$ multiplicado por constantes positivas. Em outras palavras, precisamos encontrar constantes positivas $c_1$, $c_2$ e $n_0$ tais que para todo $n \geq n_0$, temos $c_1(f(n) + g(n)) \leq \max(f(n), g(n)) \leq c_2(f(n) + g(n))$.

Para funções assintoticamente não negativas $f(n)$ e $g(n)$, sabemos que

$$
\begin{aligned}
\exists n_1, n_2: & f(n) \ge 0 & \text{para} \, n > n_1 \\\\
                  & g(n) \ge 0 & \text{para} \, n > n_2.
\end{aligned}
$$

Seja $n_0 = \max(n_1, n_2)$.

Primeiro, vamos mostrar que $\max(f(n), g(n))$ é limitado superiormente por $f(n) + g(n)$ multiplicado por uma constante positiva. Observe que $f(n) \leq f(n) + g(n)$ e $g(n) \leq f(n) + g(n)$. Portanto, $\max(f(n), g(n)) \leq f(n) + g(n)$, o que significa que podemos escolher $c_2 = 1$.

Agora, vamos mostrar que $\max(f(n), g(n))$ é limitado inferiormente por $f(n) + g(n)$ multiplicado por uma constante positiva. Observe que $f(n) + g(n) \leq 2\max(f(n), g(n))$. Portanto, $\max(f(n), g(n)) \geq \frac{1}{2}(f(n) + g(n))$, o que significa que podemos escolher $c_1 = \frac{1}{2}$.

Juntando tudo, temos que para todo $n \geq n_0$, $\frac{1}{2}(f(n) + g(n)) \leq \max(f(n), g(n)) \leq f(n) + g(n)$. Portanto, podemos concluir que $\max(f(n), g(n)) = \Theta(f(n) + g(n))$.

## 3.2-2

> Explique porque a afirmação "O tempo de execução do algoritmo $A$ é no mínimo $O(n^2)$" é sem sentido.

$T(n)$: tempo de execução do algoritmo $A$. Nós apenas estamos interessados no limite superior e inferior de $T(n)$.

A afirmação: $T(n)$ é no mínimo $O(n^2)$.

- Limite superior: Porque "$T(n)$ é no mínimo $O(n^2)$", não há informação sobre o limite superior de $T(n)$.
- Limite inferior: Suponha $f(n) = O(n^2)$, então a afirmação é $T(n) \ge f(n)$, mas $f(n)$ poderia ser qualquer função que é "menor" que $n^2$. Por exemplo, constante, $n$, etc. Portanto, não há conclusão sobre o limite inferior de $T(n)$ também.

Portanto, a afirmação "O tempo de execução do algoritmo $A$ é no mínimo $O(n^2)$" é sem sentido.

## 3.2-3

> $2^{n + 1} = O(2^n)$? $2^{2n} = O(2^n)$?

- Verdadeiro. Observe que $2^{n + 1} = 2 \times 2^n$. Podemos escolher $c \ge 2$ e $n_0 = 0$, de modo que $0 \le 2^{n + 1} \le c \times 2^n$ para todo $n \ge n_0$. Pela definição, $2^{n + 1} = O(2^n)$.

- Falso. Observe que $2^{2n} = 2^n \times 2^n = 4^n$. Não podemos encontrar qualquer $c$ e $n_0$, de modo que $0 \le 2^{2n} = 4^n \le c \times 2^n$ para todo $n \ge n_0$.

## 3.2-4

> Prove o Teorema 3.1.

O teorema afirma:

“Para quaisquer duas funções $f(n)$ e $g(n)$, temos $f(n) = \Theta(g(n))$ se e somente se $f(n) = O(g(n))$ e $f(n) = \Omega(g(n))$”.

A partir de $f = \Theta(g(n))$, temos que

$$0 \le c_1 g(n) \le f(n) \le c_2 g(n) \text{ para } n > n_0.$$

Podemos escolher as constantes daqui e usá-las nas definições de $O$ e $\Omega$ para mostrar que ambas são verdadeiras.

A partir de $f(n) = \Omega(g(n))$ e $f(n) = O(g(n))$, temos que

$$
\begin {aligned}
            & 0 \le c_3 g(n) \le f(n) & \text{ para todo } n \ge n_1 \\\\
\text{e }    & 0 \le f(n) \le c_4 g(n) & \text{ para todo } n \ge n_2.
\end {aligned}
$$

Se deixarmos $n_3 = \max(n_1, n_2)$ e mesclarmos as inequações, obtemos

$$0 \le c_3 g(n) \le f(n) \le c_4 g(n) \text{ para todo } n > n_3.$$

O que é a definição de $\Theta$.

## 3.2-5

> Prove que o tempo de execução de um algoritmo é $\Theta(g(n))$ se e somente se seu pior caso é $O(g(n))$ e seu melhor caso é $\Omega(g(n))$.

Se $T_w$ é o tempo de execução no pior caso e $T_b$ é o tempo de execução no melhor caso, sabemos que

$$
\begin{aligned}
            & 0 \le c_1g(n) \le T_b(n) & \text{ para } n > n_b \\\\
\text{e }    & 0 \le T_w(n) \le c_2g(n) & \text{ para } n > n_w.
\end{aligned}
$$

Combinando-os nós obtemos

$$0 \le c_1g(n) \le T_b(n) \le T_w(n) \le c_2g(n) \text{ para } n > \max(n_b, n_w).$$

Uma vez que o tempo de execução está limitado entre $T_b$ e $T_w$ e isso é a definição da notação $\Theta$, foi provado.

## 3.2-6

> Prove que $o(g(n)) \cap w(g(n))$ é um conjunto vazio.

Seja $f(n) = o(g(n)) \cap w(g(n))$.
Nós sabemos que para qualquer $c_1 > 0$, $c_2 > 0$,

$$
\begin{aligned}
            & \exists n_1 > 0: 0 \le f(n) < c_1g(n) \\\\
\text{e }    & \exists n_2 > 0: 0 \le c_2g(n) < f(n).
\end{aligned}
$$

Se escolhermos $n_0 = \max(n_1, n_2)$ e deixar $c_1 = c_2$, a partir da definição do problema, nós temos

$$c_1g(n) < f(n) < c_1g(n).$$

Isso não tem soluções, o que significa que a interseção é um conjunto vazio.

## 3.2-7

> Podemos estender nossa notação para o caso de dois parâmetros $n$ e $m$ que podem aumentar independentemente em diferentes taxas. Para uma determinada função $g(n, m)$, denotamos $O(g(n, m))$ o conjunto de funções:
>
> $$
> \begin{aligned}
> O(g(n, m)) = \\{f(n, m):
>   & \text{ existem constantes positivas } c, n_0, \text{ e } m_0 \\\\
>   & \text{ tais que } 0 \le f(n, m) \le cg(n, m) \\\\
>   & \text{ para todo } n \ge n_0 \text{ ou } m \ge m_0.\\}
> \end{aligned}
> $$
>
> Dê as definições correspondentes para $\Omega(g(n, m))$ e $\Theta(g(n, m))$.

$$
\begin{aligned}
\Omega(g(n, m)) = \\{ f(n, m):
  & \text{ existem constantes positivas $c$, $n_0$, e $m_0$ tais que } \\\\
  & \text{ $0 \le cg(n, m) \le f(n, m)$ para todo $n \ge n_0$ e $m \ge m_0$}.\\}
\end{aligned}
$$

$$
\begin{aligned}
\Theta(g(n, m)) = \\{ f(n, m):
  & \text{ existem constantes positivas $c_1$, $c_2$, $n_0$, e $m_0$ tais que } \\\\
  & \text{ $0 \le c_1g(n, m) \le f(n, m) \le c_2g(n, m)$ para todo $n \ge n_0$ e $m \ge m_0$}.\\}
\end{aligned}
$$

## 3.2-8

> Mostre que para quaisquer constantes reais $a$ e $b$, onde $b > 0$,
>
> $$(n + a)^b = \Theta(n^b). \tag{3.2}$$

Desenvolvendo $(n + a)^b$ pela Expansão Binomial, temos

$$(n + a)^b = C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \cdots + C_b^b n^0 a^b.$$

Além disso, sabemos que a seguinte inequação é verdadeira para qualquer polinômio quando $x \ge 1$.

$$a_0 x^0 + a_1 x^1 + \cdots + a_n x^n \le (a_0 + a_1 + \cdots + a_n) x^n.$$

Assim,

$$C_0^b n^b \le C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \cdots + C_b^b n^0 a^b \le (C_0^b + C_1^b + \cdots + C_b^b) n^b = 2^b n^b.$$

$$\implies (n + a)^b = \Theta(n^b).$$